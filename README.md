# JOYST - Journey Over Your Saved Titles

A comprehensive gaming analytics platform that extracts, processes, and visualizes Steam gaming data using modern data engineering practices and machine learning-ready infrastructure.

## Overview

JOYST is an end-to-end data platform for gaming analytics that combines:
- **Real-time data extraction** from Steam Web API
- **Distributed processing** with Apache Spark
- **Dual-database architecture** for operational and analytical workloads
- **Interactive dashboards** for gaming insights
- **ML-ready data warehouse** for game recommendations

## Architecture

```
Steam API â†’ Spark Processing â†’ PostgreSQL (Operational)
                            â†’ ClickHouse (Analytics) â†’ Metabase Dashboards
                            â†’ JSON (Backup)
```

### Core Components:
- **ğŸ® Steam API Integration**: Real-time player and game data extraction
- **âš¡ Apache Spark**: Distributed data processing and transformation
- **ğŸ˜ PostgreSQL**: Operational data store with ACID transactions
- **ğŸ”¢ ClickHouse**: Columnar analytics warehouse optimized for ML
- **ğŸ“Š Metabase**: Interactive dashboards and visualizations
- **ğŸš€ Docker + OpenTofu**: Infrastructure as Code deployment

## Prerequisites

- Docker and Docker Compose
- OpenTofu >= 1.8.0
- Python 3.10+
- Steam API Key (get from [Steam Web API](https://steamcommunity.com/dev/apikey))
- GitHub Container Registry access (for custom Spark image)

## Quick Start

### 1. Setup Development Environment

```bash
# Clone and setup
git clone <repository>
cd joyst
task setup  # Installs dependencies and initializes OpenTofu
```

### 2. Configure Steam API

Update `config.json` with your Steam credentials:

```json
{
  "steam_api_key": "YOUR_STEAM_API_KEY",
  "steam_id": "YOUR_STEAM_ID_64_BIT",
  "output_path": "data/steam_account",
  "postgres_host": "localhost",
  "postgres_port": "5432",
  "postgres_db": "joyst_dw",
  "postgres_user": "admin",
  "postgres_password": "admin",
  "clickhouse_host": "localhost",
  "clickhouse_port": "9000",
  "clickhouse_db": "gaming_analytics",
  "clickhouse_user": "admin",
  "clickhouse_password": "admin123"
}
```

### 3. Deploy Complete Infrastructure

```bash
# Deploy all services (PostgreSQL, ClickHouse, Spark, Metabase)
task infra-up

# Check all service URLs
task urls
```

### 4. Build and Run Data Pipeline

```bash
# Build custom Spark image
task docker-build

# Run Steam data extraction and processing
task spark-submit
```

### 5. Access Dashboards

```bash
# View service URLs
task urls

# Open Metabase for dashboards
open http://localhost:3000
```

## Project Structure

```
â”œâ”€â”€ .infrastructure/          # Build and deployment
â”‚   â”œâ”€â”€ build.sh             # Docker image build script  
â”‚   â””â”€â”€ docker/
â”‚       â””â”€â”€ spark.Dockerfile # Custom Spark image
â”œâ”€â”€ .github/workflows/       # CI/CD pipelines
â”œâ”€â”€ spark_jobs/              # Spark job implementations
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ steam_job.py     # Main Steam data processor
â”‚   â”‚   â”œâ”€â”€ database_writers.py # PostgreSQL & ClickHouse writers
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â”‚   â””â”€â”€ base.py          # Spark session utilities
â”‚   â””â”€â”€ tests/               # Unit tests
â”œâ”€â”€ sql/                     # Database schemas
â”‚   â”œâ”€â”€ postgres/            # PostgreSQL schemas
â”‚   â”‚   â”œâ”€â”€ 01_steam_schema.sql
â”‚   â”‚   â””â”€â”€ 02_seed_data.sql
â”‚   â””â”€â”€ clickhouse/          # ClickHouse schemas  
â”‚       â”œâ”€â”€ 03_clickhouse_schema.sql
â”‚       â””â”€â”€ 04_clickhouse_seed_data.sql
â”œâ”€â”€ tofu/                    # Infrastructure as Code
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ network/         # Docker networking
â”‚   â”‚   â”œâ”€â”€ postgres/        # PostgreSQL deployment
â”‚   â”‚   â”œâ”€â”€ clickhouse/      # ClickHouse deployment  
â”‚   â”‚   â”œâ”€â”€ spark/           # Spark cluster
â”‚   â”‚   â”œâ”€â”€ metabase/        # Dashboard service
â”‚   â”‚   â””â”€â”€ prefect/         # Workflow orchestration
â”‚   â””â”€â”€ environments/
â”‚       â””â”€â”€ local/           # Local development config
â”œâ”€â”€ config.json             # Application configuration
â””â”€â”€ Makefile                 # Development commands
```

## Services & Access Points

| Service | URL | Purpose |
|---------|-----|---------|
| ğŸ“Š **Metabase** | http://localhost:3000 | Interactive dashboards and analytics |
| âš¡ **Spark Master UI** | http://localhost:8080 | Spark cluster monitoring |
| ğŸ”¢ **ClickHouse HTTP** | http://localhost:8123 | Analytics database interface |
| ğŸ˜ **PostgreSQL** | localhost:5432 | Operational database |
| ğŸ¯ **Prefect UI** | http://localhost:4200 | Workflow orchestration |

## Data Architecture

### PostgreSQL (Operational Store)
- **Purpose**: ACID transactions, referential integrity
- **Schema**: Normalized relational model
- **Tables**: `steam_players`, `steam_games`, `steam_owned_games`
- **Use Cases**: Data consistency, operational queries

### ClickHouse (Analytics Warehouse)  
- **Purpose**: Fast analytics, ML feature store
- **Schema**: Star schema with dimension and fact tables
- **Tables**: `dim_players`, `dim_games`, `fact_gaming_sessions`
- **Use Cases**: Analytics, reporting, ML model training

## Development

### Available Task Commands

```bash
task               # Show all available commands
task setup         # Complete development environment setup
task install       # Install Python dependencies
task test          # Run test suite
task lint          # Run code linting
task format        # Format code
task docker-build  # Build custom Spark image
task infra-up      # Deploy complete infrastructure
task infra-down    # Destroy infrastructure
task spark-submit  # Run Spark data processing job
task urls          # Show all service URLs
task clean         # Clean temporary files
```

### Configuration

Configuration priority (highest to lowest):
1. **Environment variables** 
2. **Config file** (`config.json`)
3. **Default values**

Key configuration options:

| Environment Variable | Config File Key | Default | Description |
|---------------------|----------------|---------|-------------|
| `STEAM_API_KEY` | `steam_api_key` | - | Steam Web API key |
| `STEAM_ID` | `steam_id` | - | Steam user ID (64-bit) |
| `POSTGRES_HOST` | `postgres_host` | `localhost` | PostgreSQL host |
| `CLICKHOUSE_HOST` | `clickhouse_host` | `localhost` | ClickHouse host |
| `SPARK_MASTER` | `spark_master` | `spark://localhost:7077` | Spark master URL |

## Data Pipeline

### Steam Web API Integration

The platform extracts comprehensive gaming data:

- **Player Summaries**: Profile information, creation dates, activity status
- **Owned Games**: Complete game library with playtime statistics
- **Recently Played Games**: Recent gaming activity and session data
- **Game Metadata**: Titles, images, categories, and playtime metrics

### Data Processing Flow

1. **Extract**: Steam API â†’ Raw JSON data
2. **Transform**: Spark DataFrames â†’ Structured schemas  
3. **Load**: 
   - PostgreSQL â†’ Operational data (OLTP)
   - ClickHouse â†’ Analytics data (OLAP)
   - JSON â†’ Backup/debugging

### ML Features Ready

The ClickHouse warehouse includes ML-ready features:
- **Player behavior patterns**: Playtime, preferences, session duration
- **Game similarity metrics**: For collaborative filtering
- **Time-series data**: Seasonal patterns, trends
- **Feature engineering tables**: Pre-computed ML features

## Dashboards & Analytics

### Metabase Setup

1. **Access**: http://localhost:3000
2. **Add PostgreSQL datasource**:
   - Host: `localhost:5432`
   - Database: `joyst_dw`
   - User: `admin` / Password: `admin`

3. **Add ClickHouse datasource**:
   - Host: `localhost:8123` 
   - Database: `gaming_analytics`
   - User: `admin` / Password: `admin123`

### Dashboard Ideas

**Gaming Analytics**:
- Player activity timelines
- Game popularity rankings  
- Playtime distribution analysis
- Gaming session patterns

**ML Insights**:
- Player clustering and segmentation
- Game recommendation accuracy
- Seasonal gaming trends
- Player lifecycle analysis

## Infrastructure

### Local Environment Components

| Component | Purpose | Port |
|-----------|---------|------|
| **Spark Master** | Distributed processing coordinator | 8080 |
| **Spark Worker** | Processing node | - |
| **PostgreSQL** | Operational database | 5432 |
| **ClickHouse** | Analytics warehouse | 8123, 9000 |
| **Metabase** | Dashboard and BI tool | 3000 |
| **Prefect** | Workflow orchestration | 4200 |

### OpenTofu Modules

- **`network`**: Docker networking and service discovery
- **`postgres`**: Operational database with automatic schema setup
- **`clickhouse`**: Analytics warehouse with star schema
- **`spark`**: Distributed processing cluster (master + worker)  
- **`metabase`**: Dashboard service with database connections
- **`prefect`**: Workflow orchestration (future use)

## Next Steps

### Recommended Enhancements

1. **Machine Learning Models**:
   - Game recommendation engine using ClickHouse features
   - Player behavior prediction models
   - Seasonal gaming pattern analysis

2. **Advanced Analytics**:
   - Real-time streaming with Kafka
   - Advanced Spark ML pipelines  
   - A/B testing framework for recommendations

3. **Monitoring & Observability**:
   - Prometheus metrics collection
   - Grafana monitoring dashboards
   - Data quality checks and alerts

## Getting Your Steam ID

To find your Steam ID (64-bit format):

1. Visit [steamidfinder.com](https://www.steamidfinder.com/)
2. Enter your Steam profile URL (e.g., `https://steamcommunity.com/id/classquette`)
3. Copy the **SteamID64** value
4. Use this value in your `config.json`

## Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **Java Version Mismatch** | Use Docker containers instead of local Spark |
| **Database Connection Failed** | Ensure services are running: `make urls` |
| **Steam API Rate Limits** | Check API key validity and request frequency |
| **Permission Denied (ClickHouse data)** | Add `tofu/clickhouse-data/` to `.gitignore` |
| **Container Network Issues** | Restart infrastructure: `make infra-down && make infra-up` |

### Useful Commands

```bash
# Check service status
docker ps

# View service logs  
docker logs joyst-spark-master-local
docker logs joyst-postgres-local
docker logs joyst-clickhouse-local
docker logs joyst-metabase-local

# Restart specific service
docker restart joyst-metabase-local

# Clean and restart everything
task infra-down && task clean && task infra-up
```

### Development Workflow

```bash
# 1. Start development
task setup

# 2. Deploy infrastructure  
task infra-up

# 3. Make code changes
# Edit files in spark_jobs/src/

# 4. Test changes
task test && task lint

# 5. Rebuild and test pipeline
task docker-build
task spark-submit

# 6. Check results in Metabase
open http://localhost:3000
```

## License

This project is for personal use and learning purposes.

---

**JOYST** - Transform your Steam gaming data into actionable insights! ğŸ®ğŸ“Šâœ¨